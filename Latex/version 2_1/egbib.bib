 @article{Sajjan_Moore_Pan_Nagaraja_Lee_Zeng_Song_2019, title={ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation}, url={http://arxiv.org/abs/1910.02550}, abstractNote={Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp -- a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms’ performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials available on the project website: https://sites.google.com/view/cleargrasp}, note={100 citations (Semantic Scholar/arXiv) [2023-01-28]
arXiv:1910.02550 [cs, eess]
version: 2}, number={arXiv:1910.02550}, publisher={arXiv}, author={Sajjan, Shreeyak S. and Moore, Matthew and Pan, Mike and Nagaraja, Ganesh and Lee, Johnny and Zeng, Andy and Song, Shuran}, year={2019}, month={Oct} }

@ARTICLE{7913730,
  author={Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}, 
  year={2018},
  volume={40},
  number={4},
  pages={834-848},
  doi={10.1109/TPAMI.2017.2699184}}

  @inproceedings{47764,
title	= {Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning},
author	= {Andy Zeng and Shuran Song and Stefan Welker and Johnny Lee and Alberto Rodriguez and Thomas Funkhouser},
year	= {2018},
URL	= {http://vpg.cs.princeton.edu/},
booktitle	= {IEEE International Conference on Intelligent Robots and Systems (IROS)}
}



   @inproceedings{Zhang_Funkhouser_2018, title={Deep Depth Completion of a Single RGB-D Image}, ISSN={2575-7075}, DOI={10.1109/CVPR.2018.00026}, abstractNote={The goal of our work is to complete the depth channel of an RGB-D image. Commodity-grade depth cameras often fail to sense depth for shiny, bright, transparent, and distant surfaces. To address this problem, we train a deep network that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions are then combined with raw depth observations provided by the RGB-D camera to solve for depths for all pixels, including those missing in the original observation. This method was chosen over others (e.g., inpainting depths directly) as the result of extensive experiments with a new depth completion benchmark dataset, where holes are filled in training data through the rendering of surface reconstructions created from multiview RGB-D scans. Experiments with different network inputs, depth representations, loss functions, optimization methods, inpainting methods, and deep depth estimation networks show that our proposed approach provides better depth completions than these alternatives.}, booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, author={Zhang, Yinda and Funkhouser, Thomas}, year={2018}, month={Jun}, pages={175–185} }


   @article{Chen_Zhu_Papandreou_Schroff_Adam_2018, title={Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}, url={http://arxiv.org/abs/1802.02611}, abstractNote={Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at url{https://github.com/tensorflow/models/tree/master/research/deeplab}.}, note={arXiv:1802.02611 [cs]}, number={arXiv:1802.02611}, publisher={arXiv}, author={Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig}, year={2018}, month={Aug} }



 @article{Paszke_Gross_Massa_Lerer_Bradbury_Chanan_Killeen_Lin_Gimelshein_Antiga_et, title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, url={http://arxiv.org/abs/1912.01703}, abstractNote={Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.}, note={arXiv:1912.01703 [cs, stat]}, number={arXiv:1912.01703}, publisher={arXiv}, author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, year={2019}, month={Dec} }


 @article{Chang_Funkhouser_Guibas_Hanrahan_Huang_Li_Savarese_Savva_Song_Su_et, title={ShapeNet: An Information-Rich 3D Model Repository}, abstractNote={We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.}, author={Chang, Angel and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher}, year={2015}, month={Dec} }


 @article{Bostanci_Kanwal_Clark_2015, title={Augmented reality applications for cultural heritage using Kinect}, volume={5}, DOI={10.1186/s13673-015-0040-3}, abstractNote={his paper explores the use of data from the Kinect sensor for performing 
augmented reality, with emphasis on cultural heritage applications. It is shown that the
combination of depth and image correspondences from the Kinect can yield a reliable
estimate of the location and pose of the camera, though noise from the depth sensor
introduces an unpleasant jittering of the rendered view. Kalman filtering of the camera
position was found to yield a much more stable view. Results show that the system is accurate enough for in situ augmented reality applications. Skeleton tracking using Kinect data allows the appearance of participants to be augmented, and together these facilitate the development of cultural heritage applications.}, journal={Human-centric Computing and Information Sciences}, author={Bostanci, Gazi Erkan and Kanwal, Nadia and Clark, Adrian}, year={2015}, month={Jul}, pages={1–18} }


 @article{Xie_Xiang_Mousavian_Fox_2020, title={The Best of Both Modes: Separately Leveraging RGB and Depth for Unseen Object Instance Segmentation}, url={http://arxiv.org/abs/1907.13236}, abstractNote={In order to function in unstructured environments, robots need the ability to recognize unseen novel objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. We propose a novel method that separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. Our method is comprised of two stages where the first stage operates only on depth to produce rough initial masks, and the second stage refines these masks with RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is non-photorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method, trained on this dataset, can produce sharp and accurate masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping. Code, models and video can be found at https://rse-lab.cs.washington.edu/projects/unseen-object-instance-segmentation/.}, note={58 citations (Semantic Scholar/arXiv) [2023-01-30]
arXiv:1907.13236 [cs]}, number={arXiv:1907.13236}, author={Xie, Christopher and Xiang, Yu and Mousavian, Arsalan and Fox, Dieter}, year={2020}, month={Jul} }


 @article{Xie_Xiang_Mousavian_Fox_2021, title={Unseen Object Instance Segmentation for Robotic Environments}, url={http://arxiv.org/abs/2007.08073}, abstractNote={In order to function in unstructured environments, robots need the ability to recognize unseen objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. Our proposed method, UOIS-Net, separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. UOIS-Net is comprised of two stages: first, it operates only on depth to produce object instance center votes in 2D or 3D and assembles them into rough initial masks. Secondly, these initial masks are refined using RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is non-photorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method can produce sharp and accurate segmentation masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping.}, note={48 citations (Semantic Scholar/arXiv) [2023-01-28]
arXiv:2007.08073 [cs]}, number={arXiv:2007.08073}, publisher={arXiv}, author={Xie, Christopher and Xiang, Yu and Mousavian, Arsalan and Fox, Dieter}, year={2021}, month={Oct} }



 @article{Baradel_Neverova_Wolf_Mille_Mori_2018, title={Object Level Visual Reasoning in Videos}, url={http://arxiv.org/abs/1806.06157}, abstractNote={Human activity recognition is typically addressed by detecting key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this and call for models with capabilities for fine distinction and detailed comprehension of interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatiotemporal interactions in videos. The key to our approach is a choice of performing this reasoning at the object level through the integration of state of the art object detection networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC Kitchens) and achieve state of the art results on all of them. Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.}, note={arXiv:1806.06157 [cs]}, number={arXiv:1806.06157}, publisher={arXiv}, author={Baradel, Fabien and Neverova, Natalia and Wolf, Christian and Mille, Julien and Mori, Greg}, year={2018}, month={Sep} }

 @inbook{Lai_Bo_Ren_Fox_2013, address={London}, title={RGB-D Object Recognition: Features, Algorithms, and a Large Scale Benchmark}, ISBN={978-1-4471-4639-1}, url={http://link.springer.com/10.1007/978-1-4471-4640-7_9}, DOI={10.1007/978-1-4471-4640-7_9}, abstractNote={Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. We introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset consists of two parts: The RGB-D Object Dataset containing views of 300 objects organized into 51 categories, and the RGB-D Scenes Dataset containing 8 video sequences of ofﬁce and kitchen environments. The dataset has been made publicly available to the research community so as to enable rapid progress based on this promising technology. We describe the dataset collection procedure and present techniques for RGB-D object recognition and detection of objects in scenes recorded using RGB-D videos, demonstrating that combining color and depth information substantially improves quality of results.}, booktitle={Consumer Depth Cameras for Computer Vision}, publisher={Springer London}, author={Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter}, editor={Fossati, Andrea and Gall, Juergen and Grabner, Helmut and Ren, Xiaofeng and Konolige, Kurt}, year={2013}, pages={167–192}, language={en} }
 
 @article{Li_Gao_Wu_Liu_Shen_2022, title={High-quality indoor scene 3D reconstruction with RGB-D cameras: A brief review}, volume={8}, DOI={10.1007/s41095-021-0250-8}, abstractNote={High-quality 3D reconstruction is an important topic in computer graphics and computer vision with many applications, such as robotics and augmented reality. The advent of consumer RGB-D cameras has made a profound advance in indoor scene reconstruction. For the past few years, researchers have spent significant effort to develop algorithms to capture 3D models with RGB-D cameras. As depth images produced by consumer RGB-D cameras are noisy and incomplete when surfaces are shiny, bright, transparent, or far from the camera, obtaining high-quality 3D scene models is still a challenge for existing systems. We here review high-quality 3D indoor scene reconstruction methods using consumer RGB-D cameras. In this paper, we make comparisons and analyses from the following aspects: (i) depth processing methods in 3D reconstruction are reviewed in terms of enhancement and completion, (ii) ICP-based, feature-based, and hybrid methods of camera pose estimation methods are reviewed, and (iii) surface reconstruction methods are reviewed in terms of surface fusion, optimization, and completion. The performance of state-of-the-art methods is also compared and analyzed. This survey will be useful for researchers who want to follow best practices in designing new high-quality 3D reconstruction methods.}, journal={Computational Visual Media}, author={Li, Jianwei and Gao, Wei and Wu, Yihong and Liu, Yangdong and Shen, Yanfei}, year={2022}, month={Mar}, pages={1–25} }

 @article{Qi_Liu_Wu_Su_Guibas_2018, title={Frustum PointNets for 3D Object Detection From RGB-D Data}, abstractNote={In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efﬁciently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efﬁciency as well as high recall for even small objects. Beneﬁted from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.}, author={Qi, Charles R and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J}, year={2018}, language={en} }
 
 @article{Shen_Stamos_2020a, title={Frustum VoxNet for 3D object detection from RGB-D or Depth images}, url={http://arxiv.org/abs/1910.05483}, abstractNote={Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art, achieving a 2 times speedup.}, note={arXiv:1910.05483 [cs, eess]}, number={arXiv:1910.05483}, publisher={arXiv}, author={Shen, Xiaoke and Stamos, Ioannis}, year={2020}, month={Feb} }
 

 @article{Shen_Stamos_2021, title={3D Object Detection and Instance Segmentation from 3D Range and 2D Color Images}, volume={21}, rights={http://creativecommons.org/licenses/by/3.0/}, ISSN={1424-8220}, DOI={10.3390/s21041213}, abstractNote={Instance segmentation and object detection are significant problems in the fields of computer vision and robotics. We address those problems by proposing a novel object segmentation and detection system. First, we detect 2D objects based on RGB, depth only, or RGB-D images. A 3D convolutional-based system, named Frustum VoxNet, is proposed. This system generates frustums from 2D detection results, proposes 3D candidate voxelized images for each frustum, and uses a 3D convolutional neural network (CNN) based on these candidates voxelized images to perform the 3D instance segmentation and object detection. Results on the SUN RGB-D dataset show that our RGB-D-based system’s 3D inference is much faster than state-of-the-art methods, without a significant loss of accuracy. At the same time, we can provide segmentation and detection results using depth only images, with accuracy comparable to RGB-D-based systems. This is important since our methods can also work well in low lighting conditions, or with sensors that do not acquire RGB images. Finally, the use of segmentation as part of our pipeline increases detection accuracy, while providing at the same time 3D instance segmentation.}, number={44}, journal={Sensors}, publisher={Multidisciplinary Digital Publishing Institute}, author={Shen, Xiaoke and Stamos, Ioannis}, year={2021}, month={Jan}, pages={1213}, language={en} }
 
 @article{Tychola_Tsimperidis_Papakostas_2022, title={On 3D Reconstruction Using RGB-D Cameras}, volume={2}, rights={http://creativecommons.org/licenses/by/3.0/}, ISSN={2673-6470}, DOI={10.3390/digital2030022}, abstractNote={The representation of the physical world is an issue that concerns the scientific community studying computer vision, more and more. Recently, research has focused on modern techniques and methods of photogrammetry and stereoscopy with the aim of reconstructing three-dimensional realistic models with high accuracy and metric information in a short time. In order to obtain data at a relatively low cost, various tools have been developed, such as depth cameras. RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. This survey aims to describe RGB-D camera technology. We discuss the hardware and data acquisition process, in both static and dynamic environments. Depth map sensing techniques are described, focusing on their features, pros, cons, and limitations; emerging challenges and open issues to investigate are analyzed; and some countermeasures are described. In addition, the advantages, disadvantages, and limitations of RGB-D cameras in all aspects are also described critically. This survey will be useful for researchers who want to acquire, process, and analyze the data collected.}, number={33}, journal={Digital}, publisher={Multidisciplinary Digital Publishing Institute}, author={Tychola, Kyriaki A. and Tsimperidis, Ioannis and Papakostas, George A.}, year={2022}, month={Sep}, pages={401–421}, language={en} }
 @inproceedings{Wang_Zell_2021, title={Yolo+FPN: 2D and 3D Fused Object Detection With an RGB-D Camera}, ISSN={1051-4651}, DOI={10.1109/ICPR48806.2021.9413066}, abstractNote={In this paper we propose a new deep neural network system, called Yolo+FPN, which fuses both 2D and 3D object detection algorithms to achieve better real-time object detection results and faster inference speed, to be used on real robots. Finding an optimized fusion strategy to efficiently combine 3D object detection with 2D detection information is useful and challenging for both indoor and outdoor robots. In order to satisfy real-time requirements, a trade-off between accuracy and efficiency is needed. We not only have improved training and test accuracies and lower mean losses on the KITTI object detection benchmark comparing with our baseline method, but also achieve competitive average precision on 3D detection of all classes in three levels of difficulty comparing with other state-of-the-art methods. Also, we implemented Yolo+FPN system using an RGB-D camera, and compared the speed of object detection using different GPUs. For the real implementation of both indoor and outdoor scenes, we focus on person detection, which is the most challenging and important among the three classes.}, booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, author={Wang, Ya and Zell, Andreas}, year={2021}, month={Jan}, pages={4657–4664} }
 @article{Wang_Wang_Long_Gu_Li_2021, title={Recent advances in 3D object detection based on RGB-D: A survey}, volume={70}, ISSN={01419382}, DOI={10.1016/j.displa.2021.102077}, journal={Displays}, author={Wang, Yangfan and Wang, Chen and Long, Peng and Gu, Yuzong and Li, Wenfa}, year={2021}, month={Dec}, pages={102077}, language={en} }
 @article{Zhou_Fan_Cheng_Shen_Shao_2021, title={RGB-D Salient Object Detection: A Survey}, volume={7}, ISSN={2096-0433, 2096-0662}, DOI={10.1007/s41095-020-0199-z}, abstractNote={Salient object detection (SOD), which simulates the human visual perception system to locate the most attractive object(s) in a scene, has been widely applied to various computer vision tasks. Now, with the advent of depth sensors, depth maps with affluent spatial information that can be beneficial in boosting the performance of SOD, can easily be captured. Although various RGB-D based SOD models with promising performance have been proposed over the past several years, an in-depth understanding of these models and challenges in this topic remains lacking. In this paper, we provide a comprehensive survey of RGB-D based SOD models from various perspectives, and review related benchmark datasets in detail. Further, considering that the light field can also provide depth maps, we review SOD models and popular benchmark datasets from this domain as well. Moreover, to investigate the SOD ability of existing models, we carry out a comprehensive evaluation, as well as attribute-based evaluation of several representative RGB-D based SOD models. Finally, we discuss several challenges and open directions of RGB-D based SOD for future research. All collected models, benchmark datasets, source code links, datasets constructed for attribute-based evaluation, and codes for evaluation will be made publicly available at https://github.com/taozh2017/RGBDSODsurvey}, note={arXiv:2008.00230 [cs]}, number={1}, journal={Computational Visual Media}, author={Zhou, Tao and Fan, Deng-Ping and Cheng, Ming-Ming and Shen, Jianbing and Shao, Ling}, year={2021}, month={Mar}, pages={37–69} }
 @book{Rosin_Lai_Shao_Liu_2019, address={Cham}, series={Advances in Computer Vision and Pattern Recognition}, title={RGB-D Image Analysis and Processing}, ISBN={978-3-030-28602-6}, url={http://link.springer.com/10.1007/978-3-030-28603-3}, DOI={10.1007/978-3-030-28603-3}, publisher={Springer International Publishing}, year={2019}, collection={Advances in Computer Vision and Pattern Recognition}, language={en} }

 @misc{DeMF,
  doi = {10.48550/ARXIV.2207.10589},
  url = {https://arxiv.org/abs/2207.10589},
  author = {Yang, Hao and Shi, Chen and Chen, Yihong and Wang, Liwei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Boosting 3D Object Detection via Object-Focused Image Fusion},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{CAGroup3D,
  doi = {10.48550/ARXIV.2210.04264},
  url = {https://arxiv.org/abs/2210.04264},
  author = {Wang, Haiyang and Ding, Lihe and Dong, Shaocong and Shi, Shaoshuai and Li, Aoxue and Li, Jianan and Li, Zhenguo and Wang, Liwei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{FCAF3D,
  doi = {10.48550/ARXIV.2112.00322},
  url = {https://arxiv.org/abs/2112.00322},
  author = {Rukhovich, Danila and Vorontsova, Anna and Konushin, Anton},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}




 @misc{SUNRGBDweb,
  title={SUN-RGB Dataset Webpage},
  howpublished = {\url{https://rgbd.cs.princeton.edu/}},
  note = {Accessed : 2022-1-1}
}

@INPROCEEDINGS{7298655,  author={Song, Shuran and Lichtenberg, Samuel P. and Xiao, Jianxiong},  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={SUN RGB-D: A RGB-D scene understanding benchmark suite},   year={2015},  volume={},  number={},  pages={567-576},  doi={10.1109/CVPR.2015.7298655}}

@misc{Focal-L,
  doi = {10.48550/ARXIV.2107.00641},
  url = {https://arxiv.org/abs/2107.00641},
  author = {Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Focal Self-attention for Local-Global Interactions in Vision Transformers},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{EVA,
  doi = {10.48550/ARXIV.2211.07636},
  url = {https://arxiv.org/abs/2211.07636},
  author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DYHEAD,
  doi = {10.48550/ARXIV.2106.08322},
  url = {https://arxiv.org/abs/2106.08322},
  author = {Dai, Xiyang and Chen, Yinpeng and Xiao, Bin and Chen, Dongdong and Liu, Mengchen and Yuan, Lu and Zhang, Lei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Dynamic Head: Unifying Object Detection Heads with Attentions},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}
