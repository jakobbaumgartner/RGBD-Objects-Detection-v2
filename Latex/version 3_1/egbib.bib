 @article{Sajjan_Moore_Pan_Nagaraja_Lee_Zeng_Song_2019, title={ClearGrasp: 3D Shape Estimation of Transparent Objects for Manipulation}, url={http://arxiv.org/abs/1910.02550}, abstractNote={Transparent objects are a common part of everyday life, yet they possess unique visual properties that make them incredibly difficult for standard 3D sensors to produce accurate depth estimates for. In many cases, they often appear as noisy or distorted approximations of the surfaces that lie behind them. To address these challenges, we present ClearGrasp -- a deep learning approach for estimating accurate 3D geometry of transparent objects from a single RGB-D image for robotic manipulation. Given a single RGB-D image of transparent objects, ClearGrasp uses deep convolutional networks to infer surface normals, masks of transparent surfaces, and occlusion boundaries. It then uses these outputs to refine the initial depth estimates for all transparent surfaces in the scene. To train and test ClearGrasp, we construct a large-scale synthetic dataset of over 50,000 RGB-D images, as well as a real-world test benchmark with 286 RGB-D images of transparent objects and their ground truth geometries. The experiments demonstrate that ClearGrasp is substantially better than monocular depth estimation baselines and is capable of generalizing to real-world images and novel objects. We also demonstrate that ClearGrasp can be applied out-of-the-box to improve grasping algorithms’ performance on transparent objects. Code, data, and benchmarks will be released. Supplementary materials available on the project website: https://sites.google.com/view/cleargrasp}, note={100 citations (Semantic Scholar/arXiv) [2023-01-28]
arXiv:1910.02550 [cs, eess]
version: 2}, number={arXiv:1910.02550}, publisher={arXiv}, author={Sajjan, Shreeyak S. and Moore, Matthew and Pan, Mike and Nagaraja, Ganesh and Lee, Johnny and Zeng, Andy and Song, Shuran}, year={2019}, month={Oct} }

@ARTICLE{7913730,
  author={Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs}, 
  year={2018},
  volume={40},
  number={4},
  pages={834-848},
  doi={10.1109/TPAMI.2017.2699184}}

  @inproceedings{47764,
title	= {Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning},
author	= {Andy Zeng and Shuran Song and Stefan Welker and Johnny Lee and Alberto Rodriguez and Thomas Funkhouser},
year	= {2018},
URL	= {http://vpg.cs.princeton.edu/},
booktitle	= {IEEE International Conference on Intelligent Robots and Systems (IROS)}
}

 @article{Jiang_Zhao_Shi_Liu_Fu_Jia_2020, title={PointGroup: Dual-Set Point Grouping for 3D Instance Segmentation}, url={http://arxiv.org/abs/2004.01658}, DOI={10.48550/arXiv.2004.01658}, abstractNote={Instance segmentation is an important task for scene understanding. Compared to the fully-developed 2D, 3D instance segmentation for point clouds have much room to improve. In this paper, we present PointGroup, a new end-to-end bottom-up architecture, specifically focused on better grouping the points by exploring the void space between objects. We design a two-branch network to extract point features and predict semantic labels and offsets, for shifting each point towards its respective instance centroid. A clustering component is followed to utilize both the original and offset-shifted point coordinate sets, taking advantage of their complementary strength. Further, we formulate the ScoreNet to evaluate the candidate instances, followed by the Non-Maximum Suppression (NMS) to remove duplicates. We conduct extensive experiments on two challenging datasets, ScanNet v2 and S3DIS, on which our method achieves the highest performance, 63.6% and 64.0%, compared to 54.9% and 54.4% achieved by former best solutions in terms of mAP with IoU threshold 0.5.}, note={158 citations (Semantic Scholar/arXiv) [2023-01-28]
arXiv:2004.01658 [cs]}, number={arXiv:2004.01658}, publisher={arXiv}, author={Jiang, Li and Zhao, Hengshuang and Shi, Shaoshuai and Liu, Shu and Fu, Chi-Wing and Jia, Jiaya}, year={2020}, month={Apr} }




 @article{Xie_Sun_Song_Wang_Liang_Shen_Luo_2020, title={PolarMask: Single Shot Instance Segmentation with Polar Representation}, url={http://arxiv.org/abs/1909.13226}, number={arXiv:1909.13226}, publisher={arXiv}, author={Xie, Enze and Sun, Peize and Song, Xiaoge and Wang, Wenhai and Liang, Ding and Shen, Chunhua and Luo, Ping}, year={2020}, month={Feb} }

  @article{Bolya_Zhou_Xiao_Lee_2019, title={YOLACT: Real-time Instance Segmentation}, url={http://arxiv.org/abs/1904.02689}, abstractNote={We present a simple, fully-convolutional model for real-time instance segmentation that achieves 29.8 mAP on MS COCO at 33.5 fps evaluated on a single Titan Xp, which is significantly faster than any previous competitive approach. Moreover, we obtain this result after training on only one GPU. We accomplish this by breaking instance segmentation into two parallel subtasks: (1) generating a set of prototype masks and (2) predicting per-instance mask coefficients. Then we produce instance masks by linearly combining the prototypes with the mask coefficients. We find that because this process doesn’t depend on repooling, this approach produces very high-quality masks and exhibits temporal stability for free. Furthermore, we analyze the emergent behavior of our prototypes and show they learn to localize instances on their own in a translation variant manner, despite being fully-convolutional. Finally, we also propose Fast NMS, a drop-in 12 ms faster replacement for standard NMS that only has a marginal performance penalty.}, note={arXiv:1904.02689 [cs]}, number={arXiv:1904.02689}, publisher={arXiv}, author={Bolya, Daniel and Zhou, Chong and Xiao, Fanyi and Lee, Yong Jae}, year={2019}, month={Oct} }

 @article{Xie_Xiang_Mousavian_Fox_2020, title={The Best of Both Modes: Separately Leveraging RGB and Depth for Unseen Object Instance Segmentation}, url={http://arxiv.org/abs/1907.13236}, abstractNote={In order to function in unstructured environments, robots need the ability to recognize unseen novel objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. We propose a novel method that separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. Our method is comprised of two stages where the first stage operates only on depth to produce rough initial masks, and the second stage refines these masks with RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is non-photorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method, trained on this dataset, can produce sharp and accurate masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping. Code, models and video can be found at https://rse-lab.cs.washington.edu/projects/unseen-object-instance-segmentation/.}, note={58 citations (Semantic Scholar/arXiv) [2023-01-30]
arXiv:1907.13236 [cs]}, number={arXiv:1907.13236}, author={Xie, Christopher and Xiang, Yu and Mousavian, Arsalan and Fox, Dieter}, year={2020}, month={Jul} }

 @article{Lu_Chen_Ruozzi_Xiang_2022, title={Mean Shift Mask Transformer for Unseen Object Instance Segmentation}, url={http://arxiv.org/abs/2211.11679}, abstractNote={Segmenting unseen objects is a critical task in many different domains. For example, a robot may need to grasp an unseen object, which means it needs to visually separate this object from the background and/or other objects. Mean shift clustering is a common method in object segmentation tasks. However, the traditional mean shift clustering algorithm is not easily integrated into an end-to-end neural network training pipeline. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to Unseen Object Instance Segmentation, which yields a new state-of-the-art of 87.3 Boundary F-meansure on the real-world Object Clutter Indoor Dataset (OCID). Code is available at https://github.com/YoungSean/UnseenObjectsWithMeanShift}, note={0 citations (Semantic Scholar/arXiv) [2023-01-31]
arXiv:2211.11679 [cs]
version: 1}, number={arXiv:2211.11679}, publisher={arXiv}, author={Lu, Yangxiao and Chen, Yuqiao and Ruozzi, Nicholas and Xiang, Yu}, year={2022}, month={Nov} }

 @article{Xiang_Xie_Mousavian_Fox_2021, title={Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation}, url={http://arxiv.org/abs/2007.15157}, abstractNote={Segmenting unseen objects in cluttered scenes is an important skill that robots need to acquire in order to perform tasks in new environments. In this work, we propose a new method for unseen object instance segmentation by learning RGB-D feature embeddings from synthetic data. A metric learning loss function is utilized to learn to produce pixel-wise feature embeddings such that pixels from the same object are close to each other and pixels from different objects are separated in the embedding space. With the learned feature embeddings, a mean shift clustering algorithm can be applied to discover and segment unseen objects. We further improve the segmentation accuracy with a new two-stage clustering algorithm. Our method demonstrates that non-photorealistic synthetic RGB and depth images can be used to learn feature embeddings that transfer well to real-world images for unseen object instance segmentation.}, note={47 citations (Semantic Scholar/arXiv) [2023-01-28]
arXiv:2007.15157 [cs]}, number={arXiv:2007.15157}, publisher={arXiv}, author={Xiang, Yu and Xie, Christopher and Mousavian, Arsalan and Fox, Dieter}, year={2021}, month={Mar} }


 @article{Danielczuk_Matl_Gupta_Li_Lee_Mahler_Goldberg_2019, title={Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data}, url={http://arxiv.org/abs/1809.05825}, DOI={10.48550/arXiv.1809.05825}, abstractNote={The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.}, note={19 citations (Semantic Scholar/arXiv) [2023-01-30]
arXiv:1809.05825 [cs]}, number={arXiv:1809.05825}, publisher={arXiv}, author={Danielczuk, Michael and Matl, Matthew and Gupta, Saurabh and Li, Andrew and Lee, Andrew and Mahler, Jeffrey and Goldberg, Ken}, year={2019}, month={Mar} }


 @inproceedings{Kirillov_Wu_He_Girshick_2020, address={Seattle, WA, USA}, title={PointRend: Image Segmentation As Rendering}, ISBN={978-1-72817-168-5}, url={https://ieeexplore.ieee.org/document/9156402/}, DOI={10.1109/CVPR42600.2020.00982}, booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Kirillov, Alexander and Wu, Yuxin and He, Kaiming and Girshick, Ross}, year={2020}, month={Jun}, pages={9796–9805}, language={en} }


 @article{Wang_Kong_Shen_Jiang_Li_2020, title={SOLO: Segmenting Objects by Locations}, url={http://arxiv.org/abs/1912.04488}, abstractNote={We present a new, embarrassingly simple approach to instance segmentation in images. Compared to many other dense prediction tasks, e.g., semantic segmentation, it is the arbitrary number of instances that have made instance segmentation much more challenging. In order to predict a mask for each instance, mainstream approaches either follow the “detect-thensegment” strategy as used by Mask R-CNN, or predict category masks first then use clustering techniques to group pixels into individual instances. We view the task of instance segmentation from a completely new perspective by introducing the notion of ‘instance categories’, which assigns categories to each pixel within an instance according to the instance’s location and size, thus nicely converting instance mask segmentation into a classification-solvable problem. Now instance segmentation is decomposed into two classification tasks. We demonstrate a much simpler and flexible instance segmentation framework with strong performance, achieving on par accuracy with Mask R-CNN and outperforming recent singleshot instance segmenters in accuracy. We hope that this very simple and strong framework can serve as a baseline for many instance-level recognition tasks besides instance segmentation.}, note={arXiv:1912.04488 [cs]}, number={arXiv:1912.04488}, publisher={arXiv}, author={Wang, Xinlong and Kong, Tao and Shen, Chunhua and Jiang, Yuning and Li, Lei}, year={2020}, month={Jul} }


 @article{He_Gkioxari_Dollar_Girshick_2018, title={Mask R-CNN}, url={http://arxiv.org/abs/1703.06870}, DOI={10.48550/arXiv.1703.06870}, abstractNote={We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron}, note={arXiv:1703.06870 [cs]}, number={arXiv:1703.06870}, publisher={arXiv}, author={He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross}, year={2018}, month={Jan} }

  @inproceedings{Kirillov_Girshick_He_Dollar_2019, address={Long Beach, CA, USA}, title={Panoptic Feature Pyramid Networks}, ISBN={978-1-72813-293-8}, url={https://ieeexplore.ieee.org/document/8954091/}, DOI={10.1109/CVPR.2019.00656}, abstractNote={The recently introduced panoptic segmentation task has renewed our community’s interest in unifying the tasks of instance segmentation (for thing classes) and semantic segmentation (for stuff classes). However, current state-ofthe-art methods for this joint task use separate and dissimilar networks for instance and semantic segmentation, without performing any shared computation. In this work, we aim to unify these methods at the architectural level, designing a single network for both tasks. Our approach is to endow Mask R-CNN, a popular instance segmentation method, with a semantic segmentation branch using a shared Feature Pyramid Network (FPN) backbone. Surprisingly, this simple baseline not only remains effective for instance segmentation, but also yields a lightweight, topperforming method for semantic segmentation. In this work, we perform a detailed study of this minimally extended version of Mask R-CNN with FPN, which we refer to as Panoptic FPN, and show it is a robust and accurate baseline for both tasks. Given its effectiveness and conceptual simplicity, we hope our method can serve as a strong baseline and aid future research in panoptic segmentation.}, booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, publisher={IEEE}, author={Kirillov, Alexander and Girshick, Ross and He, Kaiming and Dollar, Piotr}, year={2019}, month={Jun}, pages={6392–6401}, language={en} }





   @inproceedings{Zhang_Funkhouser_2018, title={Deep Depth Completion of a Single RGB-D Image}, ISSN={2575-7075}, DOI={10.1109/CVPR.2018.00026}, abstractNote={The goal of our work is to complete the depth channel of an RGB-D image. Commodity-grade depth cameras often fail to sense depth for shiny, bright, transparent, and distant surfaces. To address this problem, we train a deep network that takes an RGB image as input and predicts dense surface normals and occlusion boundaries. Those predictions are then combined with raw depth observations provided by the RGB-D camera to solve for depths for all pixels, including those missing in the original observation. This method was chosen over others (e.g., inpainting depths directly) as the result of extensive experiments with a new depth completion benchmark dataset, where holes are filled in training data through the rendering of surface reconstructions created from multiview RGB-D scans. Experiments with different network inputs, depth representations, loss functions, optimization methods, inpainting methods, and deep depth estimation networks show that our proposed approach provides better depth completions than these alternatives.}, booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, author={Zhang, Yinda and Funkhouser, Thomas}, year={2018}, month={Jun}, pages={175–185} }


   @article{Chen_Zhu_Papandreou_Schroff_Adam_2018, title={Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}, url={http://arxiv.org/abs/1802.02611}, abstractNote={Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at url{https://github.com/tensorflow/models/tree/master/research/deeplab}.}, note={arXiv:1802.02611 [cs]}, number={arXiv:1802.02611}, publisher={arXiv}, author={Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig}, year={2018}, month={Aug} }



 @article{Paszke_Gross_Massa_Lerer_Bradbury_Chanan_Killeen_Lin_Gimelshein_Antiga_et, title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, url={http://arxiv.org/abs/1912.01703}, abstractNote={Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.}, note={arXiv:1912.01703 [cs, stat]}, number={arXiv:1912.01703}, publisher={arXiv}, author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, year={2019}, month={Dec} }


 @article{Chang_Funkhouser_Guibas_Hanrahan_Huang_Li_Savarese_Savva_Song_Su_et, title={ShapeNet: An Information-Rich 3D Model Repository}, abstractNote={We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.}, author={Chang, Angel and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher}, year={2015}, month={Dec} }


 @article{Bostanci_Kanwal_Clark_2015, title={Augmented reality applications for cultural heritage using Kinect}, volume={5}, DOI={10.1186/s13673-015-0040-3}, abstractNote={his paper explores the use of data from the Kinect sensor for performing 
augmented reality, with emphasis on cultural heritage applications. It is shown that the
combination of depth and image correspondences from the Kinect can yield a reliable
estimate of the location and pose of the camera, though noise from the depth sensor
introduces an unpleasant jittering of the rendered view. Kalman filtering of the camera
position was found to yield a much more stable view. Results show that the system is accurate enough for in situ augmented reality applications. Skeleton tracking using Kinect data allows the appearance of participants to be augmented, and together these facilitate the development of cultural heritage applications.}, journal={Human-centric Computing and Information Sciences}, author={Bostanci, Gazi Erkan and Kanwal, Nadia and Clark, Adrian}, year={2015}, month={Jul}, pages={1–18} }

@misc{u-net,
  doi = {10.48550/ARXIV.1505.04597},
  
  url = {https://arxiv.org/abs/1505.04597},
  
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{UOIS-git,
  title = {Unseen Object Instance Segmentation for Robotic Environments},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/chrisdxie/uois}}
  }

@misc{resnet-Pytorch,
title = {Resnet Pytorch},
year = {2021},
publisher = {Pytorch},
journal = {Pytorch Resnet repository},
howpublished = {\url{https://pytorch.org/hub/pytorch_vision_resnet/}}
}

 @article{He_Zhang_Ren_Sun_2015, title={Deep Residual Learning for Image Recognition}, url={http://arxiv.org/abs/1512.03385}, DOI={10.48550/arXiv.1512.03385}, abstractNote={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57 percentage error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28 percentage relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.}, note={arXiv:1512.03385 [cs]}, number={arXiv:1512.03385}, publisher={arXiv}, author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian}, year={2015}, month={Dec} }


 @article{Chang_Funkhouser_Guibas_Hanrahan_Huang_Li_Savarese_Savva_Song_Su_et al._2015, title={ShapeNet: An Information-Rich 3D Model Repository}, url={http://arxiv.org/abs/1512.03012}, abstractNote={We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.}, note={arXiv:1512.03012 [cs]}, number={arXiv:1512.03012}, publisher={arXiv}, author={Chang, Angel X. and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and Xiao, Jianxiong and Yi, Li and Yu, Fisher}, year={2015}, month={Dec} }


 @article{Song_Yu_Zeng_Chang_Savva_Funkhouser_2016, title={Semantic Scene Completion from a Single Depth Image}, url={http://arxiv.org/abs/1611.08974}, abstractNote={This paper focuses on semantic scene completion, a task for producing a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. Previous work has considered scene completion and semantic labeling of depth maps separately. However, we observe that these two problems are tightly intertwined. To leverage the coupled nature of these two tasks, we introduce the semantic scene completion network (SSCNet), an end-to-end 3D convolutional network that takes a single depth image as input and simultaneously outputs occupancy and semantic labels for all voxels in the camera view frustum. Our network uses a dilation-based 3D context module to efficiently expand the receptive field and enable 3D context learning. To train our network, we construct SUNCG - a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations. Our experiments demonstrate that the joint model outperforms methods addressing each task in isolation and outperforms alternative approaches on the semantic scene completion task.}, note={arXiv:1611.08974 [cs]}, number={arXiv:1611.08974}, publisher={arXiv}, author={Song, Shuran and Yu, Fisher and Zeng, Andy and Chang, Angel X. and Savva, Manolis and Funkhouser, Thomas}, year={2016}, month={Nov} }



 @article{Xie_Xiang_Mousavian_Fox_2021, title={Unseen Object Instance Segmentation for Robotic Environments}, url={http://arxiv.org/abs/2007.08073}, abstractNote={In order to function in unstructured environments, robots need the ability to recognize unseen objects. We take a step in this direction by tackling the problem of segmenting unseen object instances in tabletop environments. However, the type of large-scale real-world dataset required for this task typically does not exist for most robotic settings, which motivates the use of synthetic data. Our proposed method, UOIS-Net, separately leverages synthetic RGB and synthetic depth for unseen object instance segmentation. UOIS-Net is comprised of two stages: first, it operates only on depth to produce object instance center votes in 2D or 3D and assembles them into rough initial masks. Secondly, these initial masks are refined using RGB. Surprisingly, our framework is able to learn from synthetic RGB-D data where the RGB is non-photorealistic. To train our method, we introduce a large-scale synthetic dataset of random objects on tabletops. We show that our method can produce sharp and accurate segmentation masks, outperforming state-of-the-art methods on unseen object instance segmentation. We also show that our method can segment unseen objects for robot grasping.}, note={48 citations (Semantic Scholar/arXiv) [2023-01-28]
arXiv:2007.08073 [cs]}, number={arXiv:2007.08073}, publisher={arXiv}, author={Xie, Christopher and Xiang, Yu and Mousavian, Arsalan and Fox, Dieter}, year={2021}, month={Oct} }



 @article{Baradel_Neverova_Wolf_Mille_Mori_2018, title={Object Level Visual Reasoning in Videos}, url={http://arxiv.org/abs/1806.06157}, abstractNote={Human activity recognition is typically addressed by detecting key concepts like global and local motion, features related to object classes present in the scene, as well as features related to the global context. The next open challenges in activity recognition require a level of understanding that pushes beyond this and call for models with capabilities for fine distinction and detailed comprehension of interactions between actors and objects in a scene. We propose a model capable of learning to reason about semantically meaningful spatiotemporal interactions in videos. The key to our approach is a choice of performing this reasoning at the object level through the integration of state of the art object detection networks. This allows the model to learn detailed spatial interactions that exist at a semantic, object-interaction relevant level. We evaluate our method on three standard datasets (Twenty-BN Something-Something, VLOG and EPIC Kitchens) and achieve state of the art results on all of them. Finally, we show visualizations of the interactions learned by the model, which illustrate object classes and their interactions corresponding to different activity classes.}, note={arXiv:1806.06157 [cs]}, number={arXiv:1806.06157}, publisher={arXiv}, author={Baradel, Fabien and Neverova, Natalia and Wolf, Christian and Mille, Julien and Mori, Greg}, year={2018}, month={Sep} }

 @inbook{Lai_Bo_Ren_Fox_2013, address={London}, title={RGB-D Object Recognition: Features, Algorithms, and a Large Scale Benchmark}, ISBN={978-1-4471-4639-1}, url={http://link.springer.com/10.1007/978-1-4471-4640-7_9}, DOI={10.1007/978-1-4471-4640-7_9}, abstractNote={Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. We introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset consists of two parts: The RGB-D Object Dataset containing views of 300 objects organized into 51 categories, and the RGB-D Scenes Dataset containing 8 video sequences of ofﬁce and kitchen environments. The dataset has been made publicly available to the research community so as to enable rapid progress based on this promising technology. We describe the dataset collection procedure and present techniques for RGB-D object recognition and detection of objects in scenes recorded using RGB-D videos, demonstrating that combining color and depth information substantially improves quality of results.}, booktitle={Consumer Depth Cameras for Computer Vision}, publisher={Springer London}, author={Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter}, editor={Fossati, Andrea and Gall, Juergen and Grabner, Helmut and Ren, Xiaofeng and Konolige, Kurt}, year={2013}, pages={167–192}, language={en} }
 
 @article{Li_Gao_Wu_Liu_Shen_2022, title={High-quality indoor scene 3D reconstruction with RGB-D cameras: A brief review}, volume={8}, DOI={10.1007/s41095-021-0250-8}, abstractNote={High-quality 3D reconstruction is an important topic in computer graphics and computer vision with many applications, such as robotics and augmented reality. The advent of consumer RGB-D cameras has made a profound advance in indoor scene reconstruction. For the past few years, researchers have spent significant effort to develop algorithms to capture 3D models with RGB-D cameras. As depth images produced by consumer RGB-D cameras are noisy and incomplete when surfaces are shiny, bright, transparent, or far from the camera, obtaining high-quality 3D scene models is still a challenge for existing systems. We here review high-quality 3D indoor scene reconstruction methods using consumer RGB-D cameras. In this paper, we make comparisons and analyses from the following aspects: (i) depth processing methods in 3D reconstruction are reviewed in terms of enhancement and completion, (ii) ICP-based, feature-based, and hybrid methods of camera pose estimation methods are reviewed, and (iii) surface reconstruction methods are reviewed in terms of surface fusion, optimization, and completion. The performance of state-of-the-art methods is also compared and analyzed. This survey will be useful for researchers who want to follow best practices in designing new high-quality 3D reconstruction methods.}, journal={Computational Visual Media}, author={Li, Jianwei and Gao, Wei and Wu, Yihong and Liu, Yangdong and Shen, Yanfei}, year={2022}, month={Mar}, pages={1–25} }

 @article{Qi_Liu_Wu_Su_Guibas_2018, title={Frustum PointNets for 3D Object Detection From RGB-D Data}, abstractNote={In this work, we study 3D object detection from RGBD data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efﬁciently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efﬁciency as well as high recall for even small objects. Beneﬁted from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.}, author={Qi, Charles R and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J}, year={2018}, language={en} }
 
 @article{Shen_Stamos_2020a, title={Frustum VoxNet for 3D object detection from RGB-D or Depth images}, url={http://arxiv.org/abs/1910.05483}, abstractNote={Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art, achieving a 2 times speedup.}, note={arXiv:1910.05483 [cs, eess]}, number={arXiv:1910.05483}, publisher={arXiv}, author={Shen, Xiaoke and Stamos, Ioannis}, year={2020}, month={Feb} }
 

 @article{Shen_Stamos_2021, title={3D Object Detection and Instance Segmentation from 3D Range and 2D Color Images}, volume={21}, rights={http://creativecommons.org/licenses/by/3.0/}, ISSN={1424-8220}, DOI={10.3390/s21041213}, abstractNote={Instance segmentation and object detection are significant problems in the fields of computer vision and robotics. We address those problems by proposing a novel object segmentation and detection system. First, we detect 2D objects based on RGB, depth only, or RGB-D images. A 3D convolutional-based system, named Frustum VoxNet, is proposed. This system generates frustums from 2D detection results, proposes 3D candidate voxelized images for each frustum, and uses a 3D convolutional neural network (CNN) based on these candidates voxelized images to perform the 3D instance segmentation and object detection. Results on the SUN RGB-D dataset show that our RGB-D-based system’s 3D inference is much faster than state-of-the-art methods, without a significant loss of accuracy. At the same time, we can provide segmentation and detection results using depth only images, with accuracy comparable to RGB-D-based systems. This is important since our methods can also work well in low lighting conditions, or with sensors that do not acquire RGB images. Finally, the use of segmentation as part of our pipeline increases detection accuracy, while providing at the same time 3D instance segmentation.}, number={44}, journal={Sensors}, publisher={Multidisciplinary Digital Publishing Institute}, author={Shen, Xiaoke and Stamos, Ioannis}, year={2021}, month={Jan}, pages={1213}, language={en} }
 
 @article{Tychola_Tsimperidis_Papakostas_2022, title={On 3D Reconstruction Using RGB-D Cameras}, volume={2}, rights={http://creativecommons.org/licenses/by/3.0/}, ISSN={2673-6470}, DOI={10.3390/digital2030022}, abstractNote={The representation of the physical world is an issue that concerns the scientific community studying computer vision, more and more. Recently, research has focused on modern techniques and methods of photogrammetry and stereoscopy with the aim of reconstructing three-dimensional realistic models with high accuracy and metric information in a short time. In order to obtain data at a relatively low cost, various tools have been developed, such as depth cameras. RGB-D cameras are novel sensing systems that capture RGB images along with per-pixel depth information. This survey aims to describe RGB-D camera technology. We discuss the hardware and data acquisition process, in both static and dynamic environments. Depth map sensing techniques are described, focusing on their features, pros, cons, and limitations; emerging challenges and open issues to investigate are analyzed; and some countermeasures are described. In addition, the advantages, disadvantages, and limitations of RGB-D cameras in all aspects are also described critically. This survey will be useful for researchers who want to acquire, process, and analyze the data collected.}, number={33}, journal={Digital}, publisher={Multidisciplinary Digital Publishing Institute}, author={Tychola, Kyriaki A. and Tsimperidis, Ioannis and Papakostas, George A.}, year={2022}, month={Sep}, pages={401–421}, language={en} }
 @inproceedings{Wang_Zell_2021, title={Yolo+FPN: 2D and 3D Fused Object Detection With an RGB-D Camera}, ISSN={1051-4651}, DOI={10.1109/ICPR48806.2021.9413066}, abstractNote={In this paper we propose a new deep neural network system, called Yolo+FPN, which fuses both 2D and 3D object detection algorithms to achieve better real-time object detection results and faster inference speed, to be used on real robots. Finding an optimized fusion strategy to efficiently combine 3D object detection with 2D detection information is useful and challenging for both indoor and outdoor robots. In order to satisfy real-time requirements, a trade-off between accuracy and efficiency is needed. We not only have improved training and test accuracies and lower mean losses on the KITTI object detection benchmark comparing with our baseline method, but also achieve competitive average precision on 3D detection of all classes in three levels of difficulty comparing with other state-of-the-art methods. Also, we implemented Yolo+FPN system using an RGB-D camera, and compared the speed of object detection using different GPUs. For the real implementation of both indoor and outdoor scenes, we focus on person detection, which is the most challenging and important among the three classes.}, booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, author={Wang, Ya and Zell, Andreas}, year={2021}, month={Jan}, pages={4657–4664} }
 @article{Wang_Wang_Long_Gu_Li_2021, title={Recent advances in 3D object detection based on RGB-D: A survey}, volume={70}, ISSN={01419382}, DOI={10.1016/j.displa.2021.102077}, journal={Displays}, author={Wang, Yangfan and Wang, Chen and Long, Peng and Gu, Yuzong and Li, Wenfa}, year={2021}, month={Dec}, pages={102077}, language={en} }
 @article{Zhou_Fan_Cheng_Shen_Shao_2021, title={RGB-D Salient Object Detection: A Survey}, volume={7}, ISSN={2096-0433, 2096-0662}, DOI={10.1007/s41095-020-0199-z}, abstractNote={Salient object detection (SOD), which simulates the human visual perception system to locate the most attractive object(s) in a scene, has been widely applied to various computer vision tasks. Now, with the advent of depth sensors, depth maps with affluent spatial information that can be beneficial in boosting the performance of SOD, can easily be captured. Although various RGB-D based SOD models with promising performance have been proposed over the past several years, an in-depth understanding of these models and challenges in this topic remains lacking. In this paper, we provide a comprehensive survey of RGB-D based SOD models from various perspectives, and review related benchmark datasets in detail. Further, considering that the light field can also provide depth maps, we review SOD models and popular benchmark datasets from this domain as well. Moreover, to investigate the SOD ability of existing models, we carry out a comprehensive evaluation, as well as attribute-based evaluation of several representative RGB-D based SOD models. Finally, we discuss several challenges and open directions of RGB-D based SOD for future research. All collected models, benchmark datasets, source code links, datasets constructed for attribute-based evaluation, and codes for evaluation will be made publicly available at https://github.com/taozh2017/RGBDSODsurvey}, note={arXiv:2008.00230 [cs]}, number={1}, journal={Computational Visual Media}, author={Zhou, Tao and Fan, Deng-Ping and Cheng, Ming-Ming and Shen, Jianbing and Shao, Ling}, year={2021}, month={Mar}, pages={37–69} }
 @book{Rosin_Lai_Shao_Liu_2019, address={Cham}, series={Advances in Computer Vision and Pattern Recognition}, title={RGB-D Image Analysis and Processing}, ISBN={978-3-030-28602-6}, url={http://link.springer.com/10.1007/978-3-030-28603-3}, DOI={10.1007/978-3-030-28603-3}, publisher={Springer International Publishing}, year={2019}, collection={Advances in Computer Vision and Pattern Recognition}, language={en} }

 @misc{DeMF,
  doi = {10.48550/ARXIV.2207.10589},
  url = {https://arxiv.org/abs/2207.10589},
  author = {Yang, Hao and Shi, Chen and Chen, Yihong and Wang, Liwei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Boosting 3D Object Detection via Object-Focused Image Fusion},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{CAGroup3D,
  doi = {10.48550/ARXIV.2210.04264},
  url = {https://arxiv.org/abs/2210.04264},
  author = {Wang, Haiyang and Ding, Lihe and Dong, Shaocong and Shi, Shaoshuai and Li, Aoxue and Li, Jianan and Li, Zhenguo and Wang, Liwei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {CAGroup3D: Class-Aware Grouping for 3D Object Detection on Point Clouds},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{FCAF3D,
  doi = {10.48550/ARXIV.2112.00322},
  url = {https://arxiv.org/abs/2112.00322},
  author = {Rukhovich, Danila and Vorontsova, Anna and Konushin, Anton},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}




 @misc{SUNRGBDweb,
  title={SUN-RGB Dataset Webpage},
  howpublished = {\url{https://rgbd.cs.princeton.edu/}},
  note = {Accessed : 2022-1-1}
}

@INPROCEEDINGS{7298655,  author={Song, Shuran and Lichtenberg, Samuel P. and Xiao, Jianxiong},  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={SUN RGB-D: A RGB-D scene understanding benchmark suite},   year={2015},  volume={},  number={},  pages={567-576},  doi={10.1109/CVPR.2015.7298655}}

@misc{Focal-L,
  doi = {10.48550/ARXIV.2107.00641},
  url = {https://arxiv.org/abs/2107.00641},
  author = {Yang, Jianwei and Li, Chunyuan and Zhang, Pengchuan and Dai, Xiyang and Xiao, Bin and Yuan, Lu and Gao, Jianfeng},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Focal Self-attention for Local-Global Interactions in Vision Transformers},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{EVA,
  doi = {10.48550/ARXIV.2211.07636},
  url = {https://arxiv.org/abs/2211.07636},
  author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {EVA: Exploring the Limits of Masked Visual Representation Learning at Scale},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{DYHEAD,
  doi = {10.48550/ARXIV.2106.08322},
  url = {https://arxiv.org/abs/2106.08322},
  author = {Dai, Xiyang and Chen, Yinpeng and Xiao, Bin and Chen, Dongdong and Liu, Mengchen and Yuan, Lu and Zhang, Lei},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Dynamic Head: Unifying Object Detection Heads with Attentions},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}
